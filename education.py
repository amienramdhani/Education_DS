# -*- coding: utf-8 -*-
"""Education.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1V8BjjdXN4tEiRArDFdAmAyMiYju_aq_w

# Proyek Akhir: Menyelesaikan Permasalahan Perusahaan Edutech
- Nama: Muhammad Amien Ramdhani
- Email: dhanimacbull@gmail.com
- Id Dicoding: amienramdhani

# Persiapan
## Menyiapkan library yang dibutuhkan
"""

import pandas as pd
import numpy as np
import re
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.utils import resample
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import MinMaxScaler,OrdinalEncoder
from sklearn.decomposition import PCA
from sklearn.preprocessing import OneHotEncoder
import joblib
import os
from sklearn.utils import shuffle
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import GridSearchCV
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import GridSearchCV
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.metrics import classification_report, confusion_matrix
from imblearn.over_sampling import SMOTE
from sklearn.preprocessing import OrdinalEncoder

"""## Menyiapkan data yang digunakan"""

df = pd.read_csv("data.csv", delimiter=';')
df.head(5)

"""# Data Understanding"""

df.isnull().sum()

df.info()

df.describe(include="all")

"""### Memisahkan data kategorikal dan numerikal"""

new_df = df.copy()
numerical_column = [
    "Previous_qualification_grade",
    "Admission_grade",
    "Age_at_enrollment",
    "Curricular_units_1st_sem_credited",
    "Curricular_units_1st_sem_enrolled",
    "Curricular_units_1st_sem_evaluations",
    "Curricular_units_1st_sem_approved",
    "Curricular_units_1st_sem_grade",
    "Curricular_units_1st_sem_without_evaluations",
    "Curricular_units_2nd_sem_credited",
    "Curricular_units_2nd_sem_enrolled",
    "Curricular_units_2nd_sem_evaluations",
    "Curricular_units_2nd_sem_approved",
    "Curricular_units_2nd_sem_grade",
    "Curricular_units_2nd_sem_without_evaluations",
    "Unemployment_rate",
    "Inflation_rate",
    "GDP",
]

for col in numerical_column:
  new_df[col] = new_df[col].astype("float64")


new_df.info()

new_df.drop(["Curricular_units_1st_sem_without_evaluations", "Curricular_units_1st_sem_credited", "Curricular_units_2nd_sem_credited", "Curricular_units_2nd_sem_without_evaluations"], axis=1, inplace=True)

new_df.drop(["Application_order"], axis=1, inplace=True)

numerical_column = [
    "Previous_qualification_grade",
    "Admission_grade",
    "Age_at_enrollment",
    "Curricular_units_1st_sem_enrolled",
    "Curricular_units_1st_sem_evaluations",
    "Curricular_units_1st_sem_approved",
    "Curricular_units_1st_sem_grade",
    "Curricular_units_2nd_sem_enrolled",
    "Curricular_units_2nd_sem_evaluations",
    "Curricular_units_2nd_sem_approved",
    "Curricular_units_2nd_sem_grade",
    "Unemployment_rate",
    "Inflation_rate",
    "GDP",
]
new_df[numerical_column].describe()

new_df.head(10)

new_df[numerical_column].isnull().sum()

new_df.drop(["Nacionality","Educational_special_needs"], axis=1, inplace=True)

new_df.drop(["International"], axis=1, inplace=True)

categorical_columns = [
      "Marital_status",
      "Application_mode",
      "Course",
      "Daytime_evening_attendance",
      "Previous_qualification",
      "Mothers_qualification",
      "Fathers_qualification",
      "Mothers_occupation",
      "Fathers_occupation",
      "Displaced",
      "Debtor",
      "Tuition_fees_up_to_date",
      "Gender",
      "Scholarship_holder",
      "Status",
]

for col in categorical_columns:
    print(new_df[col].value_counts())

new_df.info()

new_df[categorical_columns].isna().sum()

cleaned_df = new_df.copy()
cleaned_df.describe(include="all")

"""## Exploratory Data Analysis"""

categorical_columns = [
      "Marital_status",
      "Application_mode",
      "Course",
      "Daytime_evening_attendance",
      "Previous_qualification",
      "Mothers_qualification",
      "Fathers_qualification",
      "Mothers_occupation",
      "Fathers_occupation",
      "Displaced",
      "Debtor",
      "Tuition_fees_up_to_date",
      "Gender",
      "Scholarship_holder",
      "Status",
]

fig, ax = plt.subplots(len(categorical_columns), 1,figsize=(20,40))
for i, feature in enumerate(categorical_columns):
    sns.countplot(data=cleaned_df, y=feature, ax=ax[i])
plt.show()

# Visualisasi Kategorikal
categorical_features = [
      "Marital_status",
      "Application_mode",
      "Course",
      "Daytime_evening_attendance",
      "Previous_qualification",
      "Mothers_qualification",
      "Fathers_qualification",
      "Mothers_occupation",
      "Fathers_occupation",
      "Displaced",
      "Debtor",
      "Tuition_fees_up_to_date",
      "Gender",
      "Scholarship_holder",
      "Status",
]

for col in categorical_features:
    plt.figure(figsize=(10, 4))
    sns.countplot(data=df, x=col, hue='Status', palette='Set2')
    plt.title(f'Distribusi {col} berdasarkan Status')
    plt.xticks(rotation=45)
    plt.tight_layout()
    plt.show()

numerical_features = ['Admission_grade', 'Age_at_enrollment', 'Previous_qualification_grade',
                      'Curricular_units_1st_sem_approved', 'Curricular_units_2nd_sem_approved']

for col in numerical_features:
    plt.figure(figsize=(10, 4))
    sns.boxplot(data=df, x='Status', y=col, palette='Set3')
    plt.title(f'Distribusi {col} berdasarkan Status')
    plt.tight_layout()
    plt.show()

def numerical_dis_plot(features, df, segment_feature=None, showfliers=True):
    fig, ax = plt.subplots(len(features), 1,figsize=(15,30))
    for i, feature in enumerate(features):
        if segment_feature:
            sns.boxplot(y=segment_feature, x=feature, data=df, ax=ax[i], showfliers=showfliers)
            ax[i].set_ylabel(None)
        else:
            sns.boxplot(x=feature, data=df, ax=ax[i], showfliers=showfliers)
    plt.tight_layout()
    plt.show()

numerical_dis_plot(
    features=numerical_column,
    df=cleaned_df
)

def categorical_plot(features, df, segment_feature=None):
    fig, ax = plt.subplots(len(features), 1,figsize=(10,20))
    for i, feature in enumerate(features):
        if segment_feature:
            sns.countplot(data=df, y=segment_feature, hue=feature, ax=ax[i])
        else:
            sns.countplot(data=df, x=feature, ax=ax[i])
    plt.tight_layout()
    plt.show()

categorical_plot(
    features=[
      "Marital_status",
      "Application_mode",
      "Course",
      "Daytime_evening_attendance",
      "Previous_qualification",
      "Mothers_qualification",
      "Fathers_qualification",
      "Mothers_occupation",
      "Fathers_occupation",
      "Displaced",
      "Debtor",
      "Tuition_fees_up_to_date",
      "Gender",
      "Scholarship_holder",
      "Status",
    ],
    df=cleaned_df,
    segment_feature="Status"
)

numerical_features = [
    "Previous_qualification_grade",
    "Admission_grade",
    "Age_at_enrollment",
    "Curricular_units_1st_sem_enrolled",
    "Curricular_units_1st_sem_evaluations",
    "Curricular_units_1st_sem_approved",
    "Curricular_units_1st_sem_grade",
    "Curricular_units_2nd_sem_enrolled",
    "Curricular_units_2nd_sem_evaluations",
    "Curricular_units_2nd_sem_approved",
    "Curricular_units_2nd_sem_grade",
    "Unemployment_rate",
    "Inflation_rate",
    "GDP",
]

# Hitung korelasi antar fitur numerik
corr_matrix = df[numerical_features].corr()

# Visualisasi heatmap korelasi
plt.figure(figsize=(14, 10))
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt=".2f", linewidths=0.5)
plt.title("Correlation Matrix of Numerical Features", fontsize=16)
plt.xticks(rotation=45, ha='right')
plt.tight_layout()
plt.show()

"""# Data Preprocessing

## Train-test Split
"""

train_df, test_df = train_test_split(cleaned_df, test_size=0.2, random_state=42, shuffle=True)
train_df.reset_index(drop=True, inplace=True)
test_df.reset_index(drop=True, inplace=True)

sns.countplot(data=train_df, x="Status")
plt.show()

train_df.Status.value_counts()

"""## Oversampling"""

df_majority_1 = train_df[train_df.Status == "Graduate"]
df_majority_2 = train_df[train_df.Status == "Dropout"]
df_minority = train_df[train_df.Status == "Enrolled"]

df_majority_2_us = resample(df_majority_2, n_samples=1791, random_state=42)
df_minority_us = resample(df_minority, n_samples=1791, random_state=42)

balanced_train_df = pd.concat([df_majority_1, df_majority_2_us, df_minority_us], ignore_index=True)
balanced_train_df = shuffle(balanced_train_df, random_state=42).reset_index(drop=True)

sns.countplot(data=balanced_train_df, x="Status")
plt.show()

X_train = balanced_train_df.drop(columns="Status")
y_train = balanced_train_df["Status"]

X_test = test_df.drop(columns="Status")
y_test = test_df["Status"]

"""## Encoding dan Scalling"""

def scaling(features, df, df_test=None):
    os.makedirs('model', exist_ok=True)
    df = df.copy()

    if df_test is not None:
        df_test = df_test.copy()

    for feature in features:
        scaler = MinMaxScaler()
        df[[feature]] = scaler.fit_transform(df[[feature]])
        joblib.dump(scaler, f"model/scaler_{feature}.joblib")

        if df_test is not None:
            df_test[[feature]] = scaler.transform(df_test[[feature]])

    return (df, df_test) if df_test is not None else df


def encoding(features, df, df_test=None):
    os.makedirs('model', exist_ok=True)
    df = df.copy()

    if df_test is not None:
        df_test = df_test.copy()

    # Inisialisasi OrdinalEncoder yang aman terhadap label baru (unknown)
    encoder = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)

    # Fit hanya pada data latih
    encoder.fit(df[features])
    joblib.dump(encoder, "model/ordinal_encoder.joblib")

    # Transform data latih dan uji
    df[features] = encoder.transform(df[features])
    if df_test is not None:
        df_test[features] = encoder.transform(df_test[features])

    return (df, df_test) if df_test is not None else df

numerical_columns = [
    "Previous_qualification_grade",
    "Admission_grade",
    "Age_at_enrollment",
    "Curricular_units_1st_sem_enrolled",
    "Curricular_units_1st_sem_evaluations",
    "Curricular_units_1st_sem_approved",
    "Curricular_units_1st_sem_grade",
    "Curricular_units_2nd_sem_enrolled",
    "Curricular_units_2nd_sem_evaluations",
    "Curricular_units_2nd_sem_approved",
    "Curricular_units_2nd_sem_grade",
    "Unemployment_rate",
    "Inflation_rate",
    "GDP",
]

categorical_columns = [
    "Marital_status",
    "Application_mode",
    "Course",
    "Daytime_evening_attendance",
    "Previous_qualification",
    "Mothers_qualification",
    "Fathers_qualification",
    "Mothers_occupation",
    "Fathers_occupation",
    "Displaced",
    "Debtor",
    "Tuition_fees_up_to_date",
    "Gender",
    "Scholarship_holder",
]

X_train_scaled, X_test_scaled = scaling(numerical_columns, X_train, X_test)
X_train_encoded, X_test_encoded = encoding(categorical_columns, X_train_scaled, X_test_scaled)

encoder = LabelEncoder()
encoder.fit(y_train)  # Fit dari training (balanced)
y_train_encoded = encoder.transform(y_train)
y_test_encoded = encoder.transform(y_test)
joblib.dump(encoder, "model/encoder_target.joblib")

"""#### SMOTE"""

smote = SMOTE(random_state=42)
X_smote, y_smote = smote.fit_resample(X_train_encoded, y_train_encoded)

"""## PCA (Principal Component Analysis)"""

pca_numerical_columns_1 = ['Curricular_units_1st_sem_enrolled',
    'Curricular_units_1st_sem_evaluations',
    'Curricular_units_1st_sem_approved',
    'Curricular_units_1st_sem_grade',
    'Curricular_units_2nd_sem_enrolled',
    'Curricular_units_2nd_sem_evaluations',
    'Curricular_units_2nd_sem_approved',
    'Curricular_units_2nd_sem_grade',]
pca_numerical_columns_2 = [
    'Previous_qualification_grade',
    'Admission_grade',
    'Age_at_enrollment',
    'Unemployment_rate',
    'Inflation_rate',
    'GDP',]

train_pca_df = pd.DataFrame(X_smote, columns=X_train_encoded.columns)
test_pca_df = X_test_encoded.copy()

def apply_pca_train(df, columns, n_components, pca_name):
    os.makedirs("model", exist_ok=True)

    # Visualisasi explained variance
    pca_full = PCA(n_components=len(columns), random_state=123)
    pca_full.fit(df[columns])

    var_exp = pca_full.explained_variance_ratio_.round(3)
    cum_var_exp = np.cumsum(var_exp)

    plt.figure(figsize=(8,4))
    plt.bar(range(len(columns)), var_exp, alpha=0.5, align='center', label='Individual explained variance')
    plt.step(range(len(columns)), cum_var_exp, where='mid', label='Cumulative explained variance')
    plt.ylabel('Explained variance ratio')
    plt.xlabel('Principal component index')
    plt.title(f"PCA Explained Variance: {pca_name}")
    plt.legend(loc='best')
    plt.grid(True)
    plt.tight_layout()
    plt.show()

    # Fit PCA dengan n_components yang diinginkan
    pca = PCA(n_components=n_components, random_state=123)
    pca.fit(df[columns])
    joblib.dump(pca, f"model/pca_{pca_name}.joblib")

    # Transformasi data
    princ_comp = pca.transform(df[columns])
    pc_cols = [f"pc{pca_name}_{i+1}" for i in range(n_components)]
    df[pc_cols] = pd.DataFrame(princ_comp, index=df.index, columns=pc_cols)

    # Drop kolom asli
    df.drop(columns=columns, inplace=True)
    return df


def apply_pca_test(df, columns, pca_name, n_components):
    # Load model PCA
    pca = joblib.load(f"model/pca_{pca_name}.joblib")
    princ_comp = pca.transform(df[columns])
    pc_cols = [f"pc{pca_name}_{i+1}" for i in range(n_components)]
    df[pc_cols] = pd.DataFrame(princ_comp, index=df.index, columns=pc_cols)

    # Drop kolom asli
    df.drop(columns=columns, inplace=True)
    return df

pca_numerical_columns_1 = ['Curricular_units_1st_sem_enrolled',
    'Curricular_units_1st_sem_evaluations',
    'Curricular_units_1st_sem_approved',
    'Curricular_units_1st_sem_grade',
    'Curricular_units_2nd_sem_enrolled',
    'Curricular_units_2nd_sem_evaluations',
    'Curricular_units_2nd_sem_approved',
    'Curricular_units_2nd_sem_grade',]
pca_numerical_columns_2 = [
    'Previous_qualification_grade',
    'Admission_grade',
    'Age_at_enrollment',
    'Unemployment_rate',
    'Inflation_rate',
    'GDP',]

train_pca_df = apply_pca_train(train_pca_df, pca_numerical_columns_1, 5, 1)
train_pca_df = apply_pca_train(train_pca_df, pca_numerical_columns_2, 2, 2)

test_pca_df = apply_pca_test(test_pca_df, pca_numerical_columns_1, 1, 5)
test_pca_df = apply_pca_test(test_pca_df, pca_numerical_columns_2, 2, 2)

# pca = PCA(n_components=len(pca_numerical_columns_1), random_state=123)
# pca.fit(train_pca_df[pca_numerical_columns_1])
# princ_comp = pca.transform(train_pca_df[pca_numerical_columns_1])

# var_exp = pca.explained_variance_ratio_.round(3)
# cum_var_exp = np.cumsum(var_exp)

# plt.bar(range(len(pca_numerical_columns_1)), var_exp, alpha=0.5, align='center', label='individual explained variance')
# plt.step(range(len(pca_numerical_columns_1)), cum_var_exp, where='mid', label='cumulative explained variance')
# plt.ylabel('Explained variance ratio')
# plt.xlabel('Principal component index')
# plt.legend(loc='best')
# plt.show()

# pca_1 = PCA(n_components=5, random_state=123)
# pca_1.fit(train_pca_df[pca_numerical_columns_1])
# joblib.dump(pca_1, "model/pca_{}.joblib".format(1))
# princ_comp_1 = pca_1.transform(train_pca_df[pca_numerical_columns_1])
# train_pca_df[["pc1_1", "pc1_2", "pc1_3", "pc1_4", "pc1_5"]] = pd.DataFrame(princ_comp_1, columns=["pc1_1", "pc1_2", "pc1_3", "pc1_4", "pc1_5"])
# train_pca_df.drop(columns=pca_numerical_columns_1, axis=1, inplace=True)
# train_pca_df.head()

# pca = PCA(n_components=len(pca_numerical_columns_2), random_state=123)
# pca.fit(train_pca_df[pca_numerical_columns_2])
# princ_comp = pca.transform(train_pca_df[pca_numerical_columns_2])

# var_exp = pca.explained_variance_ratio_.round(3)
# cum_var_exp = np.cumsum(var_exp)

# plt.bar(range(len(pca_numerical_columns_2)), var_exp, alpha=0.5, align='center', label='individual explained variance')
# plt.step(range(len(pca_numerical_columns_2)), cum_var_exp, where='mid', label='cumulative explained variance')
# plt.ylabel('Explained variance ratio')
# plt.xlabel('Principal component index')
# plt.legend(loc='best')
# plt.show()

# pca_2 = PCA(n_components=2, random_state=123)
# pca_2.fit(train_pca_df[pca_numerical_columns_2])
# joblib.dump(pca_2, "model/pca_{}.joblib".format(2))
# princ_comp_2 = pca_2.transform(train_pca_df[pca_numerical_columns_2])
# train_pca_df[["pc2_1", "pc2_2"]] = pd.DataFrame(princ_comp_2, columns=["pc2_1", "pc2_2"])
# train_pca_df.drop(columns=pca_numerical_columns_2, axis=1, inplace=True)
# train_pca_df.head()

# princ_comp_1_test = pca_1.transform(test_pca_df[pca_numerical_columns_1])
# test_pca_df[["pc1_1", "pc1_2", "pc1_3", "pc1_4", "pc1_5"]] = pd.DataFrame(princ_comp_1_test, columns=["pc1_1", "pc1_2", "pc1_3", "pc1_4", "pc1_5"])
# test_pca_df.drop(columns=pca_numerical_columns_1, axis=1, inplace=True)

# princ_comp_2_test = pca_2.transform(test_pca_df[pca_numerical_columns_2])
# test_pca_df[["pc2_1", "pc2_2"]] = pd.DataFrame(princ_comp_2_test, columns=["pc2_1", "pc2_2"])
# test_pca_df.drop(columns=pca_numerical_columns_2, axis=1, inplace=True)

"""# Modelling

### GridSearch
"""

param_grid = {
    "penalty": ["l1","l2"],
    "C": [0.01, 0.1, 1]
}

log_model = LogisticRegression(random_state=123)

CV_lr = GridSearchCV(estimator=log_model, param_grid=param_grid, cv=5, n_jobs=-1)
CV_lr.fit(train_pca_df, y_smote)

tree_model = DecisionTreeClassifier(random_state=123)

param_grid = {
    'max_features': ['auto', 'sqrt', 'log2'],
    'max_depth' : [5, 6, 7, 8],
    'criterion' :['gini', 'entropy']
}

CV_tree = GridSearchCV(estimator=tree_model, param_grid=param_grid, cv=5, n_jobs=-1)
CV_tree.fit(train_pca_df, y_smote)

print("best parameters: ", CV_tree.best_params_)

"""### Decision Tree"""

tree_model = DecisionTreeClassifier(
    random_state=123,
    criterion='entropy',
    max_depth=8,
    max_features='sqrt'
)

tree_model.fit(train_pca_df, y_smote)
joblib.dump(tree_model, "model/tree_model.joblib")

"""### Random Forest"""

rdf_model = RandomForestClassifier(random_state=123)

param_grid = {
    'n_estimators': [200, 500],
    'max_features': ['auto', 'sqrt', 'log2'],
    'max_depth' : [6, 7, 8],
    'criterion' :['gini', 'entropy']
}

CV_rdf = GridSearchCV(estimator=rdf_model, param_grid=param_grid, cv=5, n_jobs=-1)
CV_rdf.fit(train_pca_df, y_smote)

print("best parameters: ", CV_rdf.best_params_)

rdf_model = RandomForestClassifier(
    random_state=123,
    max_depth=8,
    n_estimators=200,
    max_features='sqrt',
    criterion='entropy',
    n_jobs=-1
)
rdf_model.fit(train_pca_df, y_smote)
joblib.dump(rdf_model, "model/rdf_model.joblib")

"""### GBoost"""

gboost_model = GradientBoostingClassifier(random_state=123)

param_grid = {
    'max_depth': [5, 8],
    'n_estimators': [200, 300],
    'learning_rate': [0.01, 0.1],
    'max_features': ['auto', 'sqrt', 'log2']
}

CV_gboost = GridSearchCV(estimator=gboost_model, param_grid=param_grid, cv=5, n_jobs=-1)
CV_gboost.fit(train_pca_df, y_smote)

print("best parameters: ", CV_gboost.best_params_)

gboost_model = GradientBoostingClassifier(
    random_state=123,
    learning_rate=0.1,
    max_depth=8,
    max_features='sqrt',
    n_estimators=300
)
gboost_model.fit(train_pca_df, y_smote)
joblib.dump(gboost_model, "model/gboost_model.joblib")

"""# Evaluation"""

def evaluating(y_pred, y_true):
    '''Evaluasi model'''
    labels=['Graduate', 'Dropout', 'Enrolled']

    print(classification_report(y_pred=y_pred, y_true=y_true))

    cnf_matrix = confusion_matrix(y_pred=y_pred, y_true=y_true, labels=labels)
    confusion_matrix_df = pd.DataFrame(cnf_matrix, labels, labels)
    sns.heatmap(confusion_matrix_df, annot=True, annot_kws={'size': 14}, fmt='d', cmap='YlGnBu')
    plt.ylabel('True label', fontsize=15)
    plt.xlabel('Predicted label', fontsize=15)
    plt.show()

    return confusion_matrix_df

y_pred_test = tree_model.predict(test_pca_df)
y_pred_test = encoder.inverse_transform(y_pred_test)

evaluating(y_pred=y_pred_test, y_true=y_test)

y_pred_test = rdf_model.predict(test_pca_df)
y_pred_test = encoder.inverse_transform(y_pred_test)

evaluating(y_pred=y_pred_test, y_true=y_test)

y_pred_test = gboost_model.predict(test_pca_df)
y_pred_test = encoder.inverse_transform(y_pred_test)

evaluating(y_pred=y_pred_test, y_true=y_test)

def plot_feature_importances(feature_importances, cols):
    features = pd.DataFrame(feature_importances, columns=['coef_value']).set_index(cols)
    features = features.sort_values(by='coef_value', ascending=False)
    top_features = features

    plt.figure(figsize=(10, 6))
    sns.barplot(x='coef_value', y=features.index, data=features)
    plt.show()
    return top_features

plot_feature_importances(gboost_model.feature_importances_, train_pca_df.columns)

!zip -r /content/file.zip /content/model

!pip install session-info

import session_info
session_info.show()

